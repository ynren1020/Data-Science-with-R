Classification is a technique to determine the extent to which a data sample will or will not be a part of a category or type.
Classification models predict categorical class labels.

Classification process ----
The classification process includes the following techniques for prediction:
Model Construction
-It is done to describe a set of predetermined classes.
-Every sample belongs to a predefined class.
-The model is represented as decision trees, classification rules, or mathematical formulae.

Model Usage
-It is done to classify unknown or future objects and to estimate the accuracy of a model.
-The accuracy rate is the percentage of the test set samples correctly classified by the model. In the case of acceptable accuracy, 
the model is used for classifying data samples with unknown class labels.


Logistic regression ----
Model the probability distribution of output y as 1 or 0. This is called as sigmoid probability (sigma)
if sigma > 0.5 then set y = 1 else set y = 0. (question - if the distribution of 0 and 1 is not equally 0.5, the criteria or threshold here should be changed?)
S(x) = 1/(1+e^x) The points lying on the sigmoid function fits are either classified as positive or negative cases. A threshold is decided for classifying the cases. (Here is how the threshold being used, professor
can use 0.7 as the threshold when quite a lot of students passing an exam, or he can use as low as 0.2 as the threshold when only a few passing the exam (with 0.5 ) )

Support Vector Machines ----
SVMs involve detecting hyperplanes (decision boundary) which segregate data into classes.
The objective of optimization is to maximize the distance between the data points (support vectors) and the hyperplane. The distance is called the margin.
The outcome (0 and 1) should be factor variable.

libary(e1701)
svm_churn <- svm(Churn ~ ., data = churn_train)
library(caret)
confusionMatrix(churn_train$Churn, predict(svm_churn), positive='1') # need specify positive='1' otherwise will predict 0
# prediction on test dataset and then check confusionMatrix

K-Nearest Neighbors (KNN) ----
K-nearest neighbors is an algorithm that classifies data points by a majority vote of its k neighbors.
It is used to assign a data point to clusters based on similarity measurement.
A new input point is classified in the category such that it has the most number of neighbors from that category.

Steps of the algorithm
-calculate the distance of the unknown data points with other training data points, i.e. choose the number of k and a distance metric.
-identify k-nearest neighbors
-use category of nearest neighbors to find the category of the new data points based on majority vote.

Choosing the value of k
When choosing the value of k, keep the following points in mind:
- if its value is too small, neighborhood is sensitive to noise points
- if its value is too large, neighborhood may include points from other classes

Computing Distance and Determining Class
For the nearest neighbor classifiers, the distance between two points is expressed in the form of Euclidean distance, which is calculated by
d = sqrt((x1-x1)^2 + (y2-y1)^2)

Naive Bayes Classifier ----
This is a probabilistic model which assumes conditional independence between features.
Given a set of features, Naive Bayes classifier is used to predict a class using probability.

Decision Tree Classification
A decision tree is a graph that makes use of branching method to demonstrate every possible outcome of a decision.

Random forest ----
A random forest can be considered an ensemble of decision trees. It builds and combines multiple decision trees to get a more accurate prediction.
Each of the decision tree models used is weak when employed on its own, but it becomes stable when put together.
They are called random because they operate by choosing predictors randomly at the time of training the model.
They are called forests because they take the output of multiple decision trees to make a decision.

Random forest algorithm steps to follow
1. Draw a random bootstrap sample of size n (randomly choose n samples from the training set).
2. Grow a decision tree from the bootstrap sample. At each node, randomly select d features.
3. Split the node using the feature that provides best split according to objective function, for instance by maximizing the information gain.
4. Repeat the steps 1 to 2 k times (k is the number of trees you want to create, using a subset of samples).
5. Aggregate the prediction by each tree for a new data point to assign the class label by majority vote (pick the group selected by most number of trees and assign new data point to that group).


Evaluating Classifier Models ----
Evaluating a model is important to know the accuracy and performance of a model.
To evaluate a model, different metrics are used:
- Confusion Matrix
- Gain and Lift Chart
- Kolmogorov Smirnov Chart
- AUC - ROC curve
- Gini Coefficient
- Concordant - Discordant Ratio
- Root Mean Squared Error

A Confusion Matrix examines all possible outcomes of prediction: true positive, true negative, false positive and false negative.
False positives are like false alarms. They are called Type I error. They occur when a negative occurrence is wrongly classified as positive.
False negatives are also called Type II error. They occur when a positive occurence is wrongly classified as negative.

The parameters calculated from a confusion matrix are:
Accuracy rate: The proportion of the total number of predictions that were right. 
Precision/Positive Predicted Value: The proportion of positive cases that were correctly identified.
Negative Predictive value: The proportion of negative cases that were correctly identified.
Recall/Sensitivity/True Positive Rate: The proportion of actual positive cases which are correctly identified.
Specificity/True Negative Rate: The proportion of actual negative cases which are correctly identified.

Accuracy rate = (TP + TN)/(TP + TN + FP + FN)
Precision (Positive Predicted Value) = TP/(TP + FP)
Recall (Sensitivity or True Positive Rate) = TP/(TP + FN)
Specificity (True negative rate) = TN/(TN + FP)

AUC - ROC curve 

Bias Variance Trade-off
Bias variance trade-off determines the model's ability to keep bias and variance to the minimum.
Bias is a measure of error on how much the predicated values differ from the actual value.
Variance indicates an algorithm's sensitivity to small changes in the training dataset.
The error in a predictive model can be summarized as a summation of bias, variance, and irreducible error.
Irreducible error, also known as noise, cannot be reduced by any algorithm.
The goal of any classification algorithm is to achieve low bias and low variance.

















